### VMware

- 빅데이터
  - 빅데이터 플랫폼 구축
    - 계정 :root 
    - pass : bitdata
    - VMware설치
    - 네트워크를 위한 설정 파일 복사
      - vmnercfg.exe - player버전과 vmnetcfg.exe를 복사한 workstation pro버전이 동일 해야한다
    - CentOS설치
    - 머신복제
      - copy(파워off후 작업)
      - `I copied it`   선택 후 진행
      - root(관리자)계정으로 선 접속 후 hadoop으로 진행
    - 하둡머신 클러스터링
      - 4대를 연결 : 머신1에서 ssh통신을 이용하여 원격접속
        - `ssh "연결할 머신의 아이피" `
          - ssh(Secure shell)
            - 서버와 클라이언트간의 텍스트 기반으로 통신 , 암호화해서 통신하기위한 프로토콜
        - 호스트명 변경 : `hostnamectl set-hostname "변경할 name"` 
      - 현재 실행중인 프로그램 확인 : `system list-nuit --type=service`     종료 :`ctrl + c`
      - 방화벽해제 : `systemctl stop firewalld`
      - 방화벽확인 : `systemctl status firewalld`
      - 서비스 비활성화 : `system disable firewalld`
      - 도메인 등록 
        - ssh통신을 하기위해 hadoop01에서 나머지 머신을 접근
        - ip로 접근하기 불편하므로 호스트명을 변경해서 작업
        - /etc/hosts 파일 수정 >> 기존내용삭제 후  `ip주소 "변경할 도메인명"` 작성
        - 네트워크 리스타트 : /etc/init.d/network restart
        - `ssh "도메인명 "`
        - 호스트 파일을 다른 머신으로 복사
          - `scp /etc/hosts root@"도메인명":/etc/hosts`
      - 네트워크 restart
  - 명령어
    - scp : 로컬  > 원격지 복사
    - cp : 로컬 > 로컬 복사
  - 디렉토리 구조
    - /home : 사용자 계정의 홈디렉토리
    - /boot : 부팅에 필요한 각종 설정파일이 위치하는곳
    - /root : root계정의 홈디렉토리
    - /bin : 리눅스에서 사용할수 있는 shell명령어가 위치하는곳
    - /sbin : 시스템관리를 위한  명령어가 위치하는곳
    - /etc : 시스템관리를 위한 설정파일이 위치하는 디렉토리(사용자정보, 파일시스템정보, 네트워크정보,.......)
    - /tmp : 시스템이 작업 중 사용하는 임시 폴더
    - /var : tmp와 유사 보통 로그 파일이 위치
    - /lib : 공통 라이브러리 파일이 저장되는 위치
    - /dev : 장치가 위치하는 디렉토리  - 리눅스는 모든 장치를 파일로 인식
    - /usr : 윈도우의 program files와 동일
    - /passwd : 사용자 계정 정보의 저장 장소
    - /rpm -Uvh 
      - u : 이전 버전이 있으면 업그레이드
      - v : 성치되는 과정을 보여주기
      - h : 설치과정에 #을 추가하기

----

### Hadoop

- hadoop 설치과정
  - jdk설치
  - hadoop설치
    - 커널버전
      - 1.x
      - 2.x
      - 3.x
  - ssh프로토콜로 암호통신을 할 수 있도록 설정하기
    - 하둡 내부에서 ssh통신을 하기 때문에 통신 할 수 있도록 설정
    - 암호키를 생성하고 공용키만 각 머신에 배포
  - hadoop설정
    - conf폴더의 설정파일을 셋팅
    - 네임노드 초기화
    - hadoop 실행
      - start -all.sh
    - 실행 후 데몬 확인
      - jps
  - hadoop 프로그래밍
    - hdfs
    - mapreduce
      - 데이터를 분류하는역활
      - mapper클래스를 상속한다,
        - mapper로 전달될 input데이터의 key, value타입 mapper의 실행결과로 출력될 output데이터의 key와 value타입을 정의
        - map메소드를 오버라이딩해서 map작업을 어떤 방식으로 처리할 것 인지 내용을 구현
          - 입력된 값을 분석하기 위한 메소드 
          - 입력된 데이터에 조건을 적용하여 원하는 데이터만 추출하기 위한 반복작업 수행
        - map메소드의  매개변수
          - 입력데이터 키, 입력값, context
          - 맵리듀스 작업을 수행하며 맵메소드의 실행결과를 프레임워크 내부에서 처리하는 다른 컴포넌트로 전달
          - 출력데이터를 기록하고 shuffle단으로 넘기고 리듀서로 내보내는 작업을 내부에서 처리할 객체 프레임워크 내부에서 기본작업을 처리하는 객체
          - 내부에서 머신들끼리 통신할때 필요한 여러가지 정보를 갖고 있는 객체
    - Reducer
      - 데이터를 집계하는 역할
      - Reducer클래스를 상속
        - reducer로 전달되는 input데이터의 key,value타입 , Reducer의 실행결과로 출력될 output데이터의  key,value타입을 명시
        - reduce메소드를 오버라이딩
          - key,value,Context객체가 전달
          - Mapper와 동일
          - reducer로 전달되는 value의 타입이 Iterable`<IntWritable>`  Iterable 즉, 입력값들이 Iterable의 형태로 전달
            -  {1,1,1,1,1,1,1,.....} 값에서 한 개의 value타입은 IntWritable이지만 여러 개가 전달되므로 반복작업을 수행해야 하고 여러 개가 전달되는 것을 Iterable의 형태로 전달받는다.
    - Driver
      - 맵리듀스를 실행하기 위한 작업을 처리하는 클래스
      -  맵리듀스를 처리하기 위한  job을 생성
      - 실제 job을 처리하기 위한 클래스가 어떤 클래스인지 등록
        - Mapper, Reducer, Driver클래스가 어떤 클래스인지 우리가 작성한 클래스를 등록
      - HDFS에서 읽고 쓸 input데이터와 output데이터의 포맷을 정의
        - 텍스트 파일의 형태로 input/output형태로 처리
      - 리듀서의 출력데이터에 대한 키와 value타입을 정의
      - hdfs에 저장된 파일을 읽고 쓸 수 있도록 path를 정의
      - job실행
    - customizing
  - hadoop eco system 설치 후 테스트
    - flume
    - sqoop
    - hive
    - pig
    - mahout
  
  
