### VMware

- 빅데이터
  - 빅데이터 플랫폼 구축
    - 계정 :root 
    - pass : bitdata
    - VMware설치
    - 네트워크를 위한 설정 파일 복사
      - vmnercfg.exe - player버전과 vmnetcfg.exe를 복사한 workstation pro버전이 동일 해야한다
    - CentOS설치
    - 머신복제
      - copy(파워off후 작업)
      - `I copied it`   선택 후 진행
      - root(관리자)계정으로 선 접속 후 hadoop으로 진행
    - 하둡머신 클러스터링
      - 4대를 연결 : 머신1에서 ssh통신을 이용하여 원격접속
        - `ssh "연결할 머신의 아이피" ` 
        - 
          - ssh(Secure shell)
            - 서버와 클라이언트간의 텍스트 기반으로 통신 , 암호화해서 통신하기위한 프로토콜
        - 호스트명 변경 : `hostnamectl set-hostname "변경할 name"` 
      - 현재 실행중인 프로그램 확인 : `system list-nuit --type=service`     종료 :`ctrl + c`
      - 방화벽해제 : `systemctl stop firewalld`
      - 방화벽확인 : `systemctl status firewalld`
      - 서비스 비활성화 : `system disable firewalld`
      - 도메인 등록 
        - ssh통신을 하기위해 hadoop01에서 나머지 머신을 접근
        - ip로 접근하기 불편하므로 호스트명을 변경해서 작업
        - /etc/hosts 파일 수정 >> 기존내용삭제 후  `ip주소 "변경할 도메인명"` 작성
        - 네트워크 리스타트 : /etc/init.d/network restart
        - `ssh "도메인명 "`
        - 호스트 파일을 다른 머신으로 복사
          - `scp /etc/hosts root@"도메인명":/etc/hosts`
      - 네트워크 restart
  - IP변경시 해야될 작업 
    - /etc/hosts파일 변경  --- root계정
    - 모든 머신 copy --- root계정
      - scp /etc/hosts root@hadoop02:/etc/hosts
    - /etc/init.d/network restart  (모든 머신) --- root계정
    - /home/hadoop/hadoop-1.2.1/conf의 core-site.xml, mapred-site.xml, hdfs-site.xml파일의 주소변경 및 모든 머신에 복사 
      - hadoop계정
    - sts프로젝트 cong폴더의 모든 설정파일의 주소변경
  - 명령어
    - scp : 로컬  > 원격지 복사
    - cp : 로컬 > 로컬 복사
  - 디렉토리 구조
    - /home : 사용자 계정의 홈디렉토리
    - /boot : 부팅에 필요한 각종 설정파일이 위치하는곳
    - /root : root계정의 홈디렉토리
    - /bin : 리눅스에서 사용할수 있는 shell명령어가 위치하는곳
    - /sbin : 시스템관리를 위한  명령어가 위치하는곳
    - /etc : 시스템관리를 위한 설정파일이 위치하는 디렉토리(사용자정보, 파일시스템정보, 네트워크정보,.......)
    - /tmp : 시스템이 작업 중 사용하는 임시 폴더
    - /var : tmp와 유사 보통 로그 파일이 위치
    - /lib : 공통 라이브러리 파일이 저장되는 위치
    - /dev : 장치가 위치하는 디렉토리  - 리눅스는 모든 장치를 파일로 인식
    - /usr : 윈도우의 program files와 동일
    - /passwd : 사용자 계정 정보의 저장 장소
    - /rpm -Uvh 
      - u : 이전 버전이 있으면 업그레이드
      - v : 성치되는 과정을 보여주기
      - h : 설치과정에 #을 추가하기

----

### Hadoop

- hadoop 설치과정
  - jdk설치
  - hadoop설치
    - 커널버전
      - 1.x
      - 2.x
      - 3.x
  - ssh프로토콜로 암호통신을 할 수 있도록 설정하기
    - 하둡 내부에서 ssh통신을 하기 때문에 통신 할 수 있도록 설정
    - 암호키를 생성하고 공용키만 각 머신에 배포
  - hadoop설정
    - conf폴더의 설정파일을 셋팅
    - 네임노드 초기화
    - hadoop 실행
      - start -all.sh
    - 실행 후 데몬 확인
      - jps
  - hadoop 프로그래밍
    - hdfs
    - mapreduce
      - 데이터를 분류하는역활
      - mapper클래스를 상속한다,
        - mapper로 전달될 input데이터의 key, value타입 mapper의 실행결과로 출력될 output데이터의 key와 value타입을 정의
        - map메소드를 오버라이딩해서 map작업을 어떤 방식으로 처리할 것 인지 내용을 구현
          - 입력된 값을 분석하기 위한 메소드 
          - 입력된 데이터에 조건을 적용하여 원하는 데이터만 추출하기 위한 반복작업 수행
        - map메소드의  매개변수
          - 입력데이터 키, 입력값, context
          - 맵리듀스 작업을 수행하며 맵메소드의 실행결과를 프레임워크 내부에서 처리하는 다른 컴포넌트로 전달
          - 출력데이터를 기록하고 shuffle단으로 넘기고 리듀서로 내보내는 작업을 내부에서 처리할 객체 프레임워크 내부에서 기본작업을 처리하는 객체
          - 내부에서 머신들끼리 통신할때 필요한 여러가지 정보를 갖고 있는 객체
    - Reducer
      - 데이터를 집계하는 역할
      - Reducer클래스를 상속
        - reducer로 전달되는 input데이터의 key,value타입 , Reducer의 실행결과로 출력될 output데이터의  key,value타입을 명시
        - reduce메소드를 오버라이딩
          - key,value,Context객체가 전달
          - Mapper와 동일
          - reducer로 전달되는 value의 타입이 Iterable`<IntWritable>`  Iterable 즉, 입력값들이 Iterable의 형태로 전달
            -  {1,1,1,1,1,1,1,.....} 값에서 한 개의 value타입은 IntWritable이지만 여러 개가 전달되므로 반복작업을 수행해야 하고 여러 개가 전달되는 것을 Iterable의 형태로 전달받는다.
    - Driver
      - 맵리듀스를 실행하기 위한 작업을 처리하는 클래스
      -  맵리듀스를 처리하기 위한  job을 생성
      - 실제 job을 처리하기 위한 클래스가 어떤 클래스인지 등록
        - Mapper, Reducer, Driver클래스가 어떤 클래스인지 우리가 작성한 클래스를 등록
      - HDFS에서 읽고 쓸 input데이터와 output데이터의 포맷을 정의
        - 텍스트 파일의 형태로 input/output형태로 처리
      - 리듀서의 출력데이터에 대한 키와 value타입을 정의
      - hdfs에 저장된 파일을 읽고 쓸 수 있도록 path를 정의
      - job실행
    - customizing
    
    - 사용자정의 옵션 활용
      - Mapper 
        - 환경설정 정보에서 사용자가 입력한 옵션정보를 읽기 위해서 setup메소드를 오버라이딩해서 처리
        - map메소드 내부에서 값에 따라 다르게 동작할 수 있도록 구현
      - Reducer
        - 기존과 동일
      - Driver
        - 사용자가  -D옵션을 이용해서 입력한 옵션값을 프로그램 안에서 사용할 수 있도록 즉 ,Mapper가 사용할수 있도록 전달
      - Configured(클래스)와 Tool(인터페이스)을 상속
        - configured는 환경설정 정보를 활용해야 하므로
        - tool은 사용자 정의 옵션을 사용하기 위해서
        - 사용자가 입력한 옵션과 input/output 경로가 입력된 기존 명령형매개변수를 구분해서 전달해야 하므로
      - run메소드를 오버라이딩
        - run메소드 내부에서 Driver에서 구현했던 모든 코드를 구현
        - GenericOptionParser를 이용해서 사용자가 입력한 옵션과 일반옵션을 분리해서 환경설정 정보에 등록되도록 처리
          - 작업이 완료시 mapper 환경 설정정보에서 값을 꺼내서 사용할수있다.
      - main메소드에서 run을 실행되도록 호출
        - run은 직접 호출하지않고 toolTunner클래스 실행될 메소드로 run을 등록해야 한다.
        - 스케줄러에 의해 호출된다.
    
    - Multiple outputs
      - 한 개의 입력 데이터를 이용해서 여러 개의 output을 만들고 싶은경우 활용
        - Mapper 
          - GenericOptionParsar작업과 동일하게 map메소드를 구성하고 구분할 수 있도록 key의 각 상황별 문자추가
        - Reducer
          - Mapper에서 넘겨준 데이터에서 구분자를 기준으로 분리해서 합산
          - 개별 output이 생성될 수 있도록 처리
            - setup
              - reducer객체가 처음 실행 될때 한번 호출 되는 메소드pl
              - multipleoutputs객체 생성
            - reduce
              - 위와 동일
              - 각 상황별로 write를 호출해서 출력 될 수 있도록 처리
              - up,down, equal 작업(stock multi예제)
            - cleanUp
              - reducer의 작업이 종료될때 한번 호출 되는 메소드
              - multipleoutputs객체를 반드시 해제
        - Driver
          - multipleoutputs로 출력될 경로를 path에 설정
          - prefix로 구분문자열을 정의
      - 보조정렬
        - 기존의 맵리듀스에서 정렬되는 기본 키 방식과 다르게 정렬 기준을 추가해서 정렬
        - 보조정렬은 키 값을 그룹핑하고 그룹핑된 레코드에 순서를 부여하면서 정렬하는 방식
        - 사용자 정의 키 작성
          - 복합키 구현
          - 클래스가 사용자정의 키
          - 정렬할 기준을 컬럼으로 갖고 있는 객체
          - 맵리듀스 프레임워크 내부에서 key/value는 네트워크에서 주고 받는 값이므로 writable타입 이어야 한다.
            - writable타입
              - 하둡내부에서 네트원크 전송을 위해 가능하도록 만들어진타입
              - 주고받을 키와 value의 데이터타입이 모두 writable의 하위클래스
          - writableComparavle상속
          - 비교기준을 멤버변수로 정의
          - 데이터를 쓰고 읽는 작업을 처리하기 위해 메소드 오버라이딩
            - 네트워크를 통해서 key가 전송되므로 사용자정의key가 네트워크로 전송될수 있도록 보내고 받는 작업을 처리, 이는 하둡의 맵리듀스 프레임워크 내부에서 지원하는 기능을 활용(writableUtils)
            - readFields오버라이딩
              - 데이터를 읽기 (역직렬화)
            - wirte오버라이딩
              - 데이터를 쓰기(직렬화)
            - compareTO오버라이딩
              - 순서를 정하기 위해서 커스텀 키를 비교하는 메소드 
          - Mapper작성하기
            - 사용자키가 outputkey로 출력될 수 있도록 정의
          - Partitioner를 정의하기
            - Reduce태스크에 분배할 수 있는  Partitioner를 정의
            - 같은 키를 갖고 있는 mapper의 출력데이터를 같은 리듀스 태스크로 보내기 위해서 해시코드를 이용하여  계산
            - Partitioner상속
              - Partitioner의 역활을 하기 위해서 상속
              - 프레임워크 내부에서 호출될 수 있도록 Partitioner를 상속
              - year를 기준으로 같은 year를 갖고 있는 데이터를 같은 리듀서에서 작업이 진행되도록 분배
              - 같은 것끼리 메모리버퍼에 쌓았다가 한꺼번에 전송
          - 그룹키
            - Reducer태스크로 보내기 전에 같은 그룹으로 그룹핑을 할 수 있도록 객체를 정의
              - 그룹키 비교기
              - ex) air데이터에서 같은 년도별로 데이터를 분류
          - 그룹키에서 같은 그룹으로 데이터 내부에서 두 번쨰 기준을 적용하여 비교 할 수 있도록 객체를 정의
            - 기준이 많으면 모두 비교 할 수 있도록  구현
            - 복합키를 기준으로 데이터를 정렬하기 위해 사용하는 객체
            - 복합키 비교기(복합키에 정의한 모든 기준을 비교 검토할 수 있도록 처리)
          - 리듀서
          - 드라이버
  - 정규표현식
    - 텍스트안에서 특정 형식의 문자열을 추출하거나 검색할때 사용하는 특수문자로 만들어진 패턴
    - 자바API
      - String class의 matches메소드
        - 매개변수로 전달한 정규표현식에 일치하는 문자열의 유무를 boolean으로 return
      - java.util.regex packge의 클래스를 활용
        - pattern class
          - 패턴을 정의 하고 작업할 수 있도록 Matcher class를  만들어내는 기능
          - compile() : 정규표현식을 인식시켜서 패턴객체를 생성
            - pattern.CASE.INSENSTIVE : 대소문자를 적용하지 않는다는 의미
          - matcher() : compile된 패턴과 문자열을 매칭시켜서 적용할 수 있는 matcher class 생성
        - Matcher
          - 패턴과 일치하는 문자열을 관리하는 클래스
          - find() : 패턴에 일치하는 문자열이 있는지 찾기(true or false return)
          - start() : 패턴과 일치하는 문자열의 start index
            - 여러개가 있으면 첫 번째 찾은 문자열의 시작index
          - end() : 패턴과 일치하는 문자열의 end index
          - group() : 패턴과 일치하는 문자열을 리턴(일치하는 문자를 추출)

------

### flume

- 데이터수집을 위한 프로그램
- 시스템로그, 파일, 웹서버로그,클릭로그와 같은 비정형데이터를 HDFS에 적재할 수 있도록 지원하는 프로그램
- 종류 - flume, chukwa, scribe, fluentd, splunk
- 구성요소
  - source : 데이터가 유입되는 지점(어떤 방식으로 유입되는지 명시)
    - flume을 통해 전송할 데이터의 소스
    - 종류 - avro, netcat, exec, spooldir, thirft, JMS
      - netcat : TCP로 수집(telbet처럼 데이터 전송)
      - JMS : 메세지를 통해 수집
  - channel : 최종목적지로 데이터를 보내기 위해 데이터를 보관하는곳
    - source와 sink사이의 연결
    - 데이터를 버퍼링하는 컴포넌트
    - 메모리, 파일, 데이터베이스 채널 이용하여 저장
  - sink : 데이터가 보내질 최종 목적지
    - 종류 : logger, avrom hdfs, hbase, elasticsearch, file_roll, thrift
      - logger : 로그로 기록(콘솔)
- 실행명령어
  - `flume-ng agent --conf conf --conf-file 설정파일명 --name agent명 `
    - --conf(-c)  : 설정파일이 있는 폴더명(현재 경로에 따라 다르게 지정)
    - --conf-file(-f) : 설정파일명 (경로를 정확하게 명시) 
    - --name(-n) : agent명 (myConsole) 
- telnet테스트
  - flume을 실행
  - 새로운 터미넣에서 telnet을 실행한 후 작업하기
  - telnet이 설치되어 있지 않으면 root계정으로 이동해서 설치
    - yum install telnet
    - systemctl start telnet.coket(서버 서비스 시작)
    - systemctl status telnet.coket(상태확인 - active상태, 23번포트 확인)
- 폴더에 저장된 타일을 읽어서 폴더로 저장
  - input/output폴더 생성
  - 설정파일 만들기
    - source : 폴더에 저장된 파일을 이동시킬 것이므로 
      - type : spoolDir 
      - 타입이 저장된 디렉토리 -  타입명은 log파일이 저장된 경우 timestamp와 같은 식별자로 저장 되도록 설정
    - sink : 다른 폴더의 파일로 저장
      - type : file_roll
      - sink.directory : 저장할 디렉토리 위치 명시
      - sink.rollinterval : 기본값 30, 30초마다 파일이 rolling된다 
        - 0으로 지정시 파일 롤링이 일어나지 않아서 이벤트가 하나의 파일로 저장
    - channel
  - 폴더에 저장된 파일을 읽어서 하둡의 HDFS에 저장
    - source 
      - type : spoolDir
    - sink
      - type : hdfs
      - hdfs.path = 저장할 hdfs경로
        - hdfs://namenode정보/flume/output
          - namenode의 호스트명 or 주소 , port정보
          - flurme/output : hdfs상의 path
          - hdfs.fileType = DaraStream
          - hdfs.writeFormat = text
          - callTimeout = 15000(대기시간)
          - hdfs.batchSize : 한번에 처리할 이벤트수
          - hdfs.useLocalTimeStamp : true로 하면 현재 날짜를 변수처럼 사용할수있다
            - hdfs://hadoop01:9000/tomcat/log/%Y/%m/%d
    - shell실행명령어를 이용해서 hdfs적재
      - source
        - type : exec
        - shell=/bin/bash -c
        - command=shell명령어
    - WAS에서 hadoop의 namenode로 전송
      - 머신 >> 머신
      - hadoop02에 was설치
      - was에 웹프로젝트를 배포
      - namenode로 전송

 
